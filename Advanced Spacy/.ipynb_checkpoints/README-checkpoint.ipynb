{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The nlp object\n",
    "\n",
    "```python\n",
    "# import the english language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# create the nlp object\n",
    "nlp = English()\n",
    "```\n",
    "\n",
    "- we can use the nlp object like a function to analyze text. Contains the preprcoessing pipeline. It also includes language specific rules used for tokenizing the text into words and punctuation.\n",
    "\n",
    "#### The Doc object\n",
    "- When we process a text with the nlp object, spacy creates a Doc object - short for \"document\". \n",
    "\n",
    "```python\n",
    "# created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# iterate over tokens in a doc\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    \n",
    "token = doc[1]\n",
    "\n",
    "print(token.text)\n",
    "```\n",
    "\n",
    "#### The Span object\n",
    "- A span object is a slice of the document consisting of one or more tokens. It's only a view of the doc and doesn't contain any data itself. To create a span we can use python slicing\n",
    "\n",
    "```python\n",
    "# a slice from the Doc is a span object\n",
    "span = doc[1:4]\n",
    "\n",
    "print(span.text)\n",
    "```\n",
    "\n",
    "#### Lexical attributes\n",
    "\n",
    "```python\n",
    "doc = nlp(\"It costs $5.\")\n",
    "\n",
    "print(\"Index: \", [token.i for token in doc])\n",
    "print(\"Text: \", [token.text for token in doc])\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])\n",
    "```\n",
    "\n",
    "### Statistical models\n",
    "- Enables spacy to predict linguistic attributes in context e.g Part-of-speech tags, syntatic dependencies, named entities trained on labeled example texts.\n",
    "- `en_core_web_sm` package is a small English model that supports all core capabilities and is trained on web text.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "```\n",
    "\n",
    "#### Predicting Part-of-speech tags\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# process the text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)\n",
    "```\n",
    "\n",
    "- In spacy attributes that returns string ends with an underscore, attributes without the underscore return an ID.\n",
    "\n",
    "#### Predicting the Syntactic Dependencies\n",
    "- In addition to the part-of-the-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
    "- Spacy uses a standardized labels scheme.\n",
    "- The \"dep underscore\" attribute returns the predicted dependency label.\n",
    "\n",
    "<img src=\"syntactic_dependencies.JPG\" width=\"350\" title=\"syntatic_dependencies\">\n",
    "\n",
    "- The pronoun she is a nominal subject attached to the verb - in this case, to \"ate\". The noun \"pizza\" is a direct object attached to the verb \"ate\". It is eaten by the subject \"she\".\n",
    "- The determiner \"the\", also known as an article, is attached to the noun \"pizza\".\n",
    "\n",
    "#### Predicting Named Entities\n",
    "\n",
    "```python\n",
    "# process a text\n",
    "doc = nlp(u\"Apple is looking at buying U.K startup for $1 billion\")\n",
    "\n",
    "#iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "```\n",
    "\n",
    "#### The explain method\n",
    "- To get definitions for the most common tags and labels.\n",
    "\n",
    "```python\n",
    "spacy.explain(\"GPE\")\n",
    "spacy.explain(\"NER\")\n",
    "spacy.explain(\"dobj\")\n",
    "```\n",
    "\n",
    "\n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
